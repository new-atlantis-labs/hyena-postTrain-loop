{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "539c2a7b-c714-4b55-af65-4a3b4442a9b6",
   "metadata": {},
   "source": [
    "# HyenaDNA Model Training Pipeline for Genomic Data\n",
    "This notebook demonstrates our automated pipeline for training advanced AI models on genomic data using AWS's cloud infrastructure. The process combines AWS's genomics services (HealthOmics) with state-of-the-art machine learning capabilities (SageMaker) to create powerful models for genomic analysis.\n",
    "\n",
    "\n",
    "![notebook_diagram](../assets/notebook_diagram.jpeg)\n",
    "\n",
    "\n",
    "## Overview of the Process\n",
    "1. Data Access: We securely access genomic data from AWS HealthOmics\n",
    "2. Training Setup: We configure and launch the model training process\n",
    "3. Monitoring: We track the training progress in real-time\n",
    "4. Deployment: We make the trained model available for predictions\n",
    "\n",
    "## Summary of Benefits\n",
    "This automated pipeline provides several key advantages:\n",
    "1. **Efficiency**: Automated processes reduce manual work and potential errors\n",
    "2. **Scalability**: Can handle increasing amounts of genomic data\n",
    "3. **Cost-effectiveness**: Resources are used only when needed\n",
    "4. **Reproducibility**: The process can be repeated consistently\n",
    "5. **Monitoring**: Clear visibility into the training process\n",
    "6. **Security**: Maintains data security through AWS's infrastructure"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cc890ba-a717-4650-aa4b-55e2be7d86e5",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 1. Initial Setup\n",
    "First, we install the necessary software tools and import our custom modules that manage the training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ff5d28d6-dc1e-4def-88e4-307ce0365ac2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['notebooks', 'scripts', '.virtual_documents', '.Trash-1000', 'lost+found', 'assets', 'README.md', 'requirements.txt', '.sparkmagic', '.ipynb_checkpoints']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.chdir('/home/ec2-user/SageMaker')\n",
    "print(os.listdir(os.getcwd()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dc39d347-9391-4f87-9471-9e008ce0b723",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/home/ec2-user/SageMaker/scripts')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b7fe67bf-7f22-4d9a-ad98-9d9c03273697",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/ec2-user/.config/sagemaker/config.yaml\n"
     ]
    }
   ],
   "source": [
    "%pip install -qU pip\n",
    "%pip install -qU sagemaker boto3 awscli ipywidgets\n",
    "\n",
    "from session_handler import SessionHandler\n",
    "from data_handler import HealthOmicsHandler\n",
    "from training_handler import TrainingHandler\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34714945-5302-4b89-a66e-d0d397959eea",
   "metadata": {},
   "source": [
    "## 2. AWS Session Initialization\n",
    "Here we set up secure connections to AWS services. Think of this as logging into all the necessary AWS systems we'll need to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "434e5fda-2ca5-4dce-93f2-0fb70abe2050",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:botocore.credentials:Found credentials from IAM Role: BaseNotebookInstanceEc2InstanceRole\n",
      "INFO:sagemaker:Created S3 bucket: sagemaker-us-west-2-051581800907\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using region: us-west-2\n",
      "Using S3 bucket: sagemaker-us-west-2-051581800907\n"
     ]
    }
   ],
   "source": [
    "# Initialize sessions\n",
    "session_handler = SessionHandler()\n",
    "session_info = session_handler.get_session_info\n",
    "\n",
    "print(f\"Using region: {session_info['region_name']}\")\n",
    "print(f\"Using S3 bucket: {session_info['bucket']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4b79355-bb53-4e3a-ae21-e315f3161b7d",
   "metadata": {},
   "source": [
    "## 3. Data Access Configuration\n",
    "Now we set up access to our genomic data stored in AWS HealthOmics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfbe212b-5032-411f-913c-165354948563",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store configuration\n",
    "store_id = \"8721910603\"  # replace with your store id\n",
    "store_type = \"reference\"   # or \"reference\"\n",
    "\n",
    "# Setup data access\n",
    "omics_handler = HealthOmicsHandler(region_name=session_info['region_name'])\n",
    "access_info = omics_handler.get_store_access(\n",
    "    store_id=store_id,\n",
    "    store_type=store_type\n",
    ")\n",
    "\n",
    "# Get and display required IAM policy\n",
    "iam_policy = omics_handler.get_iam_policy(access_info[\"s3_arn\"], access_info[\"key_arn\"])\n",
    "print(f\"Required IAM policy for {store_type} store:\")\n",
    "print(json.dumps(iam_policy, indent=2))\n",
    "\n",
    "print(f\"\\nData URI for {store_type} store:\")\n",
    "print(access_info[\"data_uri\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efb20200-6e6d-4936-86a6-4bf0d8763168",
   "metadata": {},
   "source": [
    "## 4. Training Configuration\n",
    "Here we define the specifications for our training process. These settings determine:\n",
    "- How the model will learn from the data\n",
    "- How long it will train\n",
    "- How much computing power it will use\n",
    "- What metrics we'll track to measure success\n",
    "\n",
    "Key Parameters Explained:\n",
    "- Model ID: The base AI model we're using (HyenaDNA)\n",
    "- Epochs: How many times the model will process the entire dataset\n",
    "- Batch Size: How much data the model processes at once\n",
    "- Learning Rate: How quickly the model adjusts its understanding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38b56f2d-518e-4de5-b10f-8f179978ad86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model and job configuration\n",
    "MODEL_ID = 'LongSafari/hyenadna-small-32k-seqlen-hf'\n",
    "TRAINING_JOB_NAME = f'hyenaDNA-pretraining-{store_type}'  # Updated to include store type\n",
    "EXPERIMENT_NAME = \"hyenaDNA-pretraining-v2\"\n",
    "\n",
    "# Training hyperparameters\n",
    "hyperparameters = {\n",
    "    \"species\": \"mouse\",\n",
    "    \"epochs\": 150,\n",
    "    \"model_checkpoint\": MODEL_ID,\n",
    "    \"max_length\": 32_000,\n",
    "    \"batch_size\": 4,\n",
    "    \"learning_rate\": 6e-4,\n",
    "    \"weight_decay\": 0.1,\n",
    "    \"log_level\": \"INFO\",\n",
    "    \"log_interval\": 100\n",
    "}\n",
    "\n",
    "# Metrics to track\n",
    "metric_definitions = [\n",
    "    {\"Name\": \"epoch\", \"Regex\": \"Epoch: ([0-9.]*)\"},\n",
    "    {\"Name\": \"step\", \"Regex\": \"Step: ([0-9.]*)\"},\n",
    "    {\"Name\": \"train_loss\", \"Regex\": \"Train Loss: ([0-9.e-]*)\"},\n",
    "    {\"Name\": \"train_perplexity\", \"Regex\": \"Train Perplexity: ([0-9.e-]*)\"},\n",
    "    {\"Name\": \"eval_loss\", \"Regex\": \"Eval Average Loss: ([0-9.e-]*)\"},\n",
    "    {\"Name\": \"eval_perplexity\", \"Regex\": \"Eval Perplexity: ([0-9.e-]*)\"}\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8a72c95-5fac-476c-b9a8-63be2d301b70",
   "metadata": {},
   "source": [
    "## 5. Model Training\n",
    "This is where the actual learning happens. We're using Amazon SageMaker, AWS's machine learning platform, to:\n",
    "- Distribute the training across powerful GPU computers\n",
    "- Automatically manage the computing resources\n",
    "- Track the model's learning progress\n",
    "- Ensure efficient use of computing resources\n",
    "\n",
    "The training process will continue automatically until completion, with all progress metrics being logged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f3a0f2f-d51d-4966-80f2-e288dd5b0c94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup and run training\n",
    "training_handler = TrainingHandler(session_info)\n",
    "training_handler.setup_training(\n",
    "    experiment_name=EXPERIMENT_NAME,\n",
    "    base_job_name=TRAINING_JOB_NAME,\n",
    "    hyperparameters=hyperparameters,\n",
    "    metric_definitions=metric_definitions\n",
    ")\n",
    "\n",
    "print(f\"Starting training using data from {store_type} store...\")\n",
    "training_job_name = training_handler.train(data_uri=access_info[\"data_uri\"])\n",
    "print(f\"Training job name: {training_job_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbcb343f-5371-4222-8073-27ff3d6a05d5",
   "metadata": {},
   "source": [
    "## 6. Training Progress Visualization\n",
    "We use TensorBoard, an interactive visualization tool, to monitor the training progress in real-time. This allows us to see:\n",
    "- How quickly the model is learning\n",
    "- Whether it's improving its accuracy\n",
    "- If there are any issues that need attention\n",
    "\n",
    "The visualization provides both technical teams and business stakeholders with clear insights into the training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00f8a76f-d8c4-46db-a37a-7074ea9398ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_profile = \"your-profile-name\"  # replace with your sagemaker studio profile name\n",
    "\n",
    "\n",
    "tensorboard_url = session_handler.setup_tensorboard(\n",
    "    training_job_name=training_job_name,\n",
    "    user_profile=user_profile\n",
    ")\n",
    "print(f\"TensorBoard URL: {tensorboard_url}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d5558a7-2c00-4c70-b42d-ec033091c24d",
   "metadata": {},
   "source": [
    "--------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e106f14-8089-4e88-a5e8-7bd393ff7466",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.pytorch.model import PyTorchModel\n",
    "from sagemaker.serializers import JSONSerializer\n",
    "from sagemaker.deserializers import JSONDeserializer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dd05ad3-55fa-4adb-a677-3626107bd300",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure endpoint\n",
    "endpoint_name = f'hyenaDNA-pretrained-{store_type}-ep'  # Updated to include store type\n",
    "pytorch_deployment_uri = f\"763104351884.dkr.ecr.{session_info['region_name']}.amazonaws.com/pytorch-inference:2.2.0-gpu-py310-cu118-ubuntu20.04-sagemaker\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e7102cc-8181-4609-b71e-f3a4d55062a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model\n",
    "hyenaDNAModel = PyTorchModel(\n",
    "    model_data=training_handler.estimator.model_data,\n",
    "    role=session_info['role'],\n",
    "    image_uri=pytorch_deployment_uri,\n",
    "    entry_point=\"inference.py\",\n",
    "    source_dir=\"scripts/\",\n",
    "    sagemaker_session=session_info['sagemaker_session'],\n",
    "    name=endpoint_name,\n",
    "    env = {\n",
    "        'MMS_MAX_REQUEST_SIZE': '2000000000',\n",
    "        'MMS_MAX_RESPONSE_SIZE': '2000000000',\n",
    "        'MMS_DEFAULT_RESPONSE_TIMEOUT': '900',\n",
    "        'TS_MAX_RESPONSE_SIZE': '2000000000',\n",
    "        'TS_MAX_REQUEST_SIZE': '2000000000',\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec4f7cb9-84aa-4f19-bec2-229f494093ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment configuration\n",
    "env = {\n",
    "    'SAGEMAKER_MODEL_SERVER_TIMEOUT': '7200',\n",
    "    'TS_MAX_RESPONSE_SIZE': '2000000000',\n",
    "    'TS_MAX_REQUEST_SIZE': '2000000000',\n",
    "    'MMS_MAX_RESPONSE_SIZE': '2000000000',\n",
    "    'MMS_MAX_REQUEST_SIZE': '2000000000'\n",
    "}\n",
    "\n",
    "# Deploy the model\n",
    "print(f\"Deploying model to endpoint: {endpoint_name}\")\n",
    "predictor = hyenaDNAModel.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type=\"ml.g5.8xlarge\",\n",
    "    endpoint_name=endpoint_name,\n",
    "    env=env,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ce55602-f318-47be-9b50-a2cd3644018f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load test data\n",
    "print(\"Loading test data...\")\n",
    "sample_genome_data = []\n",
    "with open(\"./sample_mouse_data.json\") as file:\n",
    "    for line in file:\n",
    "        sample_genome_data.append(json.loads(line))\n",
    "\n",
    "print(f\"Loaded {len(sample_genome_data)} test samples\")\n",
    "\n",
    "# Make prediction\n",
    "print(\"Making prediction...\")\n",
    "data = [sample_genome_data[0]]\n",
    "predictor.serializer = JSONSerializer()\n",
    "predictor.deserializer = JSONDeserializer()\n",
    "embeddings = predictor.predict(data=data)\n",
    "\n",
    "print(\"Prediction complete!\")\n",
    "print(f\"Embedding shape or size: {len(embeddings) if isinstance(embeddings, list) else embeddings.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e87a87a3-e5ed-44cd-99f3-34b11b223c75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup endpoint\n",
    "print(f\"Cleaning up endpoint: {endpoint_name}\")\n",
    "predictor.delete_endpoint()\n",
    "print(\"Cleanup complete!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow2_p310",
   "language": "python",
   "name": "conda_tensorflow2_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
